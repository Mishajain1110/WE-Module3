{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###For file input by ChatGPT"
      ],
      "metadata": {
        "id": "NYvFOw5pxVkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_from_text(text: str, start_words: list[str], chain_length: int, num_generated: int) -> str:\n",
        "    # Preprocess the text\n",
        "    text = text.lower()\n",
        "    words = text.split()\n",
        "\n",
        "    # Build the Markov chain\n",
        "    markov_chain = {}\n",
        "    for i in range(len(words) - chain_length):\n",
        "        key = tuple(words[i:i + chain_length])\n",
        "        value = words[i + chain_length]\n",
        "        if key in markov_chain:\n",
        "            markov_chain[key].append(value)\n",
        "        else:\n",
        "            markov_chain[key] = [value]\n",
        "\n",
        "    # Generate the sentence\n",
        "    sentence = ' '.join(start_words)\n",
        "    current_words = start_words.copy()\n",
        "    for _ in range(num_generated - chain_length):\n",
        "        key = tuple(current_words)\n",
        "        if key in markov_chain:\n",
        "            next_word = random.choice(markov_chain[key])\n",
        "            sentence += ' ' + next_word\n",
        "            current_words = current_words[1:] + [next_word]\n",
        "        else:\n",
        "            break\n",
        "    return sentence\n",
        "\n",
        "# Read the contents of the text file\n",
        "with open('text_corpus.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Define the list of test cases\n",
        "test_cases = [\n",
        "    {\"start_words\": [\"machine\", \"learning\"], \"chain_length\": 2, \"num_generated\": 10},\n",
        "    {\"start_words\": [\"natural\", \"language\"], \"chain_length\": 2, \"num_generated\": 15},\n",
        "    {\"start_words\": [\"neural\", \"networks\"], \"chain_length\": 2, \"num_generated\": 10},\n",
        "    {\"start_words\": [\"reinforcement\", \"learning\"], \"chain_length\": 2, \"num_generated\": 12}\n",
        "]\n",
        "\n",
        "# Generate sentences for each test case\n",
        "for i, case in enumerate(test_cases, start=1):\n",
        "    start_words = case[\"start_words\"]\n",
        "    chain_length = case[\"chain_length\"]\n",
        "    num_generated = case[\"num_generated\"]\n",
        "\n",
        "    generated_sentence = generate_from_text(text, start_words, chain_length, num_generated)\n",
        "    print(f\"Generated sentence for Test Case {i}: {generated_sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxZhtBFruRRI",
        "outputId": "56ac10f3-15a0-4fef-8800-342d4ce98490"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sentence for Test Case 1: machine learning is a programming language used in in data\n",
            "Generated sentence for Test Case 2: natural language processing (nlp) deals with understanding and generating human language by employing algorithms and\n",
            "Generated sentence for Test Case 3: neural networks are computational models inspired by the human brain.\n",
            "Generated sentence for Test Case 4: reinforcement learning is a subset of artificial intelligence focused on training agents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JX-wqi_FzqYj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}